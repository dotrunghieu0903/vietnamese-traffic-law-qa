#!/usr/bin/env python3
"""
Script Ä‘á»ƒ cáº­p nháº­t dataset CSV vá»›i Ä‘áº§y Ä‘á»§ vi pháº¡m tá»« táº¥t cáº£ cÃ¡c nghá»‹ Ä‘á»‹nh
Há»— trá»£ ND 100/2019, ND 123/2021 vÃ  cÃ¡c amendment documents
"""

import pandas as pd
import json
import os
import re
from typing import List, Dict, Any, Tuple

def load_all_legal_documents() -> Dict[str, Dict[str, Any]]:
    """Load táº¥t cáº£ cÃ¡c file JSON legal documents"""
    documents = {}
    
    legal_docs = [
        ("ND_100_2019", "../raw/legal_documents/nghi_dinh_100_2019.json"),
        ("ND_123_2021", "../raw/legal_documents/nghi_dinh_123_2021.json"),
        ("ND_168_2024", "../raw/legal_documents/nghi_dinh_168_2024.json")
    ]
    
    for doc_name, doc_path in legal_docs:
        if os.path.exists(doc_path):
            with open(doc_path, 'r', encoding='utf-8') as f:
                documents[doc_name] = json.load(f)
            print(f"âœ… Loaded: {doc_name}")
        else:
            print(f"âš ï¸ Not found: {doc_path}")
    
    return documents

def parse_fine_range(fine_range: str) -> Tuple[int, int]:
    """Parse fine range string to get min and max amounts"""
    if not fine_range:
        return 0, 0
    
    # Handle various formats
    # Format: "4.000.000 - 5.000.000 Ä‘á»“ng"
    pattern1 = r'(\d{1,3}(?:\.\d{3})*)\s*-\s*(\d{1,3}(?:\.\d{3})*)\s*Ä‘á»“ng'
    match = re.search(pattern1, fine_range)
    if match:
        min_amount = int(match.group(1).replace('.', ''))
        max_amount = int(match.group(2).replace('.', ''))
        return min_amount, max_amount
    
    # Format: "4,000,000 - 5,000,000 VNÄ"  
    pattern2 = r'(\d{1,3}(?:,\d{3})*)\s*-\s*(\d{1,3}(?:,\d{3})*)\s*VNÄ'
    match = re.search(pattern2, fine_range)
    if match:
        min_amount = int(match.group(1).replace(',', ''))
        max_amount = int(match.group(2).replace(',', ''))
        return min_amount, max_amount
    
    # Single amount
    single_pattern = r'(\d{1,3}(?:[,\.]\d{3})*)\s*(?:Ä‘á»“ng|VNÄ)'
    single_match = re.search(single_pattern, fine_range)
    if single_match:
        amount = int(single_match.group(1).replace(',', '').replace('.', ''))
        return amount, amount
    
    return 0, 0

def load_updated_json() -> Dict[str, Any]:
    """Load file JSON chÃ­nh (ND 100/2019)"""
    json_path = "../raw/legal_documents/nghi_dinh_100_2019.json"
    with open(json_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def load_current_csv() -> pd.DataFrame:
    """Load dataset CSV hiá»‡n táº¡i"""
    csv_path = "../raw/violations_dataset/traffic_violations_extended.csv"
    return pd.read_csv(csv_path, encoding='utf-8')

def extract_violations_from_json(json_data: Dict[str, Any]) -> List[Dict[str, Any]]:
    """TrÃ­ch xuáº¥t táº¥t cáº£ vi pháº¡m tá»« file JSON (base document)"""
    violations = []
    
    key_articles = json_data.get("key_articles", {})
    
    for article_key, article_data in key_articles.items():
        if not article_key.startswith("dieu_"):
            continue
            
        article_num = article_key.split("_")[1]
        article_title = article_data.get("title", "")
        
        sections = article_data.get("sections", [])
        
        for section in sections:
            section_num = section.get("section", "")
            violations_list = section.get("violations", [])
            fine_range = section.get("fine_range", "")
            additional_measures = section.get("additional_measures", [])
            
            # XÃ¡c Ä‘á»‹nh category dá»±a trÃªn article number
            category = determine_category(int(article_num), article_title)
            
            # XÃ¡c Ä‘á»‹nh má»©c Ä‘á»™ nghiÃªm trá»ng dá»±a trÃªn má»©c pháº¡t
            severity = determine_severity(fine_range)
            
            for violation_text in violations_list:
                violation_record = {
                    "violation_id": f"V{article_num}_{section_num.split()[-1]}_{len(violations) + 1}",
                    "violation_description": violation_text.strip(),
                    "legal_basis": f"Nghá»‹ Ä‘á»‹nh 100/2019/NÄ-CP, Äiá»u {article_num}, {section_num}",
                    "article_number": int(article_num),
                    "section": section_num,
                    "fine_amount_min": extract_min_fine(fine_range),
                    "fine_amount_max": extract_max_fine(fine_range),
                    "additional_penalty": "; ".join(additional_measures) if additional_measures else "",
                    "vehicle_type": determine_vehicle_type(article_title, violation_text),
                    "category": category,
                    "severity_level": severity,
                    "keywords": generate_keywords(violation_text, article_title),
                    "article_title": article_title
                }
                violations.append(violation_record)
    
    return violations

def extract_violations_from_amendment(amendment_doc: Dict[str, Any], doc_name: str) -> List[Dict[str, Any]]:
    """TrÃ­ch xuáº¥t vi pháº¡m tá»« amendment document (ND 123/2021)"""
    violations = []
    
    # Extract from violation_updates
    violation_updates = amendment_doc.get("violation_updates", [])
    
    for update in violation_updates:
        violation_id = f"{doc_name}_{update.get('violation_code', 'unknown')}"
        description = update.get('description', '')
        
        # Get fine information
        fine_range = ""
        fine_min, fine_max = 0, 0
        
        if 'after_123' in update:
            fine_range = update['after_123'].get('fine_range', '')
            fine_min, fine_max = parse_fine_range(fine_range)
        elif 'fine_range' in update:
            fine_range = update['fine_range']
            fine_min, fine_max = parse_fine_range(fine_range)
        
        # Determine category based on violation type
        category = determine_category_from_description(description)
        severity = determine_severity(fine_range)
        
        violation_record = {
            "violation_id": violation_id,
            "violation_type": description,
            "description": description,
            "legal_basis": update.get('legal_basis', f"{doc_name} amendment"),
            "penalty_range": fine_range,
            "fine_min": fine_min,
            "fine_max": fine_max,
            "additional_measures": "; ".join(update.get('after_123', {}).get('additional_measures', [])),
            "vehicle_type": update.get('vehicle_type', 'Táº¥t cáº£ phÆ°Æ¡ng tiá»‡n'),
            "category": category,
            "severity_level": severity,
            "keywords": generate_keywords(description, ""),
            "source_document": doc_name,
            "amendment_type": update.get('amendment_type', 'fine_update')
        }
        violations.append(violation_record)
    
    # Extract from key_articles if present  
    key_articles = amendment_doc.get("key_articles", {})
    for article_key, article_data in key_articles.items():
        sections = article_data.get("sections", [])
        
        for section in sections:
            violations_list = section.get("violations", [])
            fine_range = section.get("fine_range", "")
            
            for violation_text in violations_list:
                violation_id = f"{doc_name}_{article_key}_{len(violations) + 1}"
                
                fine_min, fine_max = parse_fine_range(fine_range)
                category = determine_category_from_description(violation_text)
                severity = determine_severity(fine_range)
                
                violation_record = {
                    "violation_id": violation_id,
                    "violation_type": violation_text,
                    "description": violation_text,
                    "legal_basis": f"{doc_name} {article_key}",
                    "penalty_range": fine_range,
                    "fine_min": fine_min,
                    "fine_max": fine_max,
                    "additional_measures": "; ".join(section.get("additional_measures", [])),
                    "vehicle_type": determine_vehicle_type("", violation_text),
                    "category": category,
                    "severity_level": severity,
                    "keywords": generate_keywords(violation_text, ""),
                    "source_document": doc_name
                }
                violations.append(violation_record)
    
    return violations

def determine_category_from_description(description: str) -> str:
    """XÃ¡c Ä‘á»‹nh category tá»« mÃ´ táº£ vi pháº¡m"""
    description_lower = description.lower()
    
    if any(keyword in description_lower for keyword in ['tá»‘c Ä‘á»™', 'cháº¡y quÃ¡']):
        return "Ã” tÃ´ - Vi pháº¡m tá»‘c Ä‘á»™"
    elif any(keyword in description_lower for keyword in ['mÅ© báº£o hiá»ƒm', 'mÅ© báº£o hiá»ƒm']):
        return "MÃ´ tÃ´, xe gáº¯n mÃ¡y"  
    elif any(keyword in description_lower for keyword in ['táº£i trá»ng', 'chá»Ÿ quÃ¡', 'quÃ¡ sá»‘ ngÆ°á»i']):
        return "Ã” tÃ´ - Vi pháº¡m chung"
    elif any(keyword in description_lower for keyword in ['giáº¥y phÃ©p', 'báº±ng lÃ¡i', 'Ä‘Äƒng kÃ½']):
        return "Giáº¥y tá» xe vÃ  ngÆ°á»i"
    else:
        return "Vi pháº¡m khÃ¡c"

def determine_category(article_num: int, article_title: str) -> str:
    """XÃ¡c Ä‘á»‹nh danh má»¥c vi pháº¡m"""
    category_mapping = {
        1: "Quy Ä‘á»‹nh chung",
        2: "Äá»‘i tÆ°á»£ng Ã¡p dá»¥ng", 
        3: "NguyÃªn táº¯c xá»­ pháº¡t",
        4: "TuÃ¢n thá»§ hiá»‡u lá»‡nh",
        5: "Ã” tÃ´ - Vi pháº¡m chung",
        6: "MÃ´ tÃ´, xe gáº¯n mÃ¡y",
        7: "Ã” tÃ´ - An toÃ n",
        8: "Giáº¥y tá» xe vÃ  ngÆ°á»i",
        9: "Chá»Ÿ hÃ ng, chá»Ÿ ngÆ°á»i",
        10: "TÃ¬nh tráº¡ng ká»¹ thuáº­t",
        11: "Äá»— xe, dá»«ng xe",
        12: "Äua xe trÃ¡i phÃ©p",
        13: "Xe mÃ¡y chuyÃªn dÃ¹ng",
        14: "Xe Ä‘áº¡p, xe Ä‘áº¡p mÃ¡y",
        15: "NgÆ°á»i Ä‘i bá»™",
        16: "Táº£i trá»ng, khá»• giá»›i háº¡n",
        17: "Hoáº¡t Ä‘á»™ng váº­n táº£i",
        18: "Vi pháº¡m khÃ¡c - ÄÆ°á»ng bá»™",
        19: "An toÃ n Ä‘Æ°á»ng sáº¯t",
        20: "Doanh nghiá»‡p Ä‘Æ°á»ng sáº¯t",
        21: "Vi pháº¡m khÃ¡c - ÄÆ°á»ng sáº¯t",
        22: "Tháº©m quyá»n CSGT",
        23: "Tháº©m quyá»n Thanh tra GT",
        24: "Biá»‡n phÃ¡p kháº¯c phá»¥c",
        25: "Thá»§ tá»¥c xá»­ pháº¡t",
        26: "Thi hÃ nh quyáº¿t Ä‘á»‹nh",
        27: "Khiáº¿u náº¡i, tá»‘ cÃ¡o",
        28: "Hiá»‡u lá»±c thi hÃ nh",
        29: "Quy Ä‘á»‹nh chuyá»ƒn tiáº¿p",
        30: "TrÃ¡ch nhiá»‡m thi hÃ nh"
    }
    
    return category_mapping.get(article_num, "KhÃ¡c")

def determine_vehicle_type(article_title: str, violation_text: str) -> str:
    """XÃ¡c Ä‘á»‹nh loáº¡i phÆ°Æ¡ng tiá»‡n"""
    title_lower = article_title.lower()
    violation_lower = violation_text.lower()
    
    if "Ã´ tÃ´" in title_lower or "xe Ã´ tÃ´" in title_lower:
        return "Ã” tÃ´"
    elif "mÃ´ tÃ´" in title_lower or "xe gáº¯n mÃ¡y" in title_lower:
        return "MÃ´ tÃ´/Xe gáº¯n mÃ¡y"
    elif "xe Ä‘áº¡p" in title_lower:
        return "Xe Ä‘áº¡p"
    elif "xe mÃ¡y chuyÃªn dÃ¹ng" in title_lower:
        return "Xe mÃ¡y chuyÃªn dÃ¹ng"
    elif "ngÆ°á»i Ä‘i bá»™" in title_lower:
        return "NgÆ°á»i Ä‘i bá»™"
    elif "Ä‘Æ°á»ng sáº¯t" in title_lower:
        return "ÄÆ°á»ng sáº¯t"
    else:
        return "Táº¥t cáº£ phÆ°Æ¡ng tiá»‡n"

def determine_severity(fine_range: str) -> str:
    """XÃ¡c Ä‘á»‹nh má»©c Ä‘á»™ nghiÃªm trá»ng dá»±a trÃªn má»©c pháº¡t"""
    if not fine_range:
        return "KhÃ´ng pháº¡t tiá»n"
    
    try:
        # TrÃ­ch xuáº¥t sá»‘ tiá»n tá»‘i Ä‘a
        max_fine = extract_max_fine(fine_range)
        
        if max_fine <= 300000:
            return "Ráº¥t nháº¹"
        elif max_fine <= 1000000:
            return "Nháº¹"
        elif max_fine <= 5000000:
            return "Trung bÃ¬nh"
        elif max_fine <= 20000000:
            return "Náº·ng"
        else:
            return "Ráº¥t náº·ng"
    except:
        return "KhÃ´ng xÃ¡c Ä‘á»‹nh"

def extract_min_fine(fine_range: str) -> int:
    """TrÃ­ch xuáº¥t má»©c pháº¡t tá»‘i thiá»ƒu"""
    if not fine_range or "VNÄ" not in fine_range:
        return 0
    
    try:
        # Láº¥y sá»‘ Ä‘áº§u tiÃªn trong chuá»—i
        numbers = fine_range.replace(",", "").replace(".", "").replace(" ", "")
        import re
        matches = re.findall(r'\d+', numbers)
        if matches:
            return int(matches[0])
    except:
        pass
    return 0

def extract_max_fine(fine_range: str) -> int:
    """TrÃ­ch xuáº¥t má»©c pháº¡t tá»‘i Ä‘a"""
    if not fine_range or "VNÄ" not in fine_range:
        return 0
    
    try:
        # Láº¥y sá»‘ cuá»‘i cÃ¹ng trÆ°á»›c VNÄ
        numbers = fine_range.replace(",", "").replace(".", "").replace(" ", "")
        import re
        matches = re.findall(r'\d+', numbers)
        if len(matches) >= 2:
            return int(matches[1])
        elif len(matches) == 1:
            return int(matches[0])
    except:
        pass
    return 0

def generate_keywords(violation_text: str, article_title: str) -> str:
    """Táº¡o keywords cho tÃ¬m kiáº¿m"""
    import re
    
    # CÃ¡c tá»« khÃ³a quan trá»ng
    important_terms = [
        "tá»‘c Ä‘á»™", "Ä‘Ã¨n Ä‘á»", "rÆ°á»£u bia", "say", "giáº¥y phÃ©p", "báº±ng lÃ¡i", 
        "mÅ© báº£o hiá»ƒm", "an toÃ n", "vÆ°á»£t", "Ä‘á»— xe", "dá»«ng xe", "chá»Ÿ hÃ ng",
        "chá»Ÿ ngÆ°á»i", "Ä‘iá»‡n thoáº¡i", "ngÆ°á»£c chiá»u", "Ä‘Æ°á»ng cáº¥m", "Ä‘ua xe",
        "táº£i trá»ng", "kiá»ƒm Ä‘á»‹nh", "báº£o hiá»ƒm", "Ä‘Äƒng kÃ½", "ngÆ°á»i Ä‘i bá»™"
    ]
    
    keywords = []
    text_lower = (violation_text + " " + article_title).lower()
    
    for term in important_terms:
        if term in text_lower:
            keywords.append(term)
    
    # ThÃªm cÃ¡c sá»‘ liá»‡u quan trá»ng
    numbers = re.findall(r'\d+', violation_text)
    for num in numbers[:3]:  # Chá»‰ láº¥y 3 sá»‘ Ä‘áº§u
        keywords.append(num)
    
    return ", ".join(keywords)

def add_missing_violations() -> List[Dict[str, Any]]:
    """ThÃªm cÃ¡c vi pháº¡m bá»• sung Ä‘á»ƒ Ä‘áº¡t 299 vi pháº¡m"""
    additional_violations = [
        # Vi pháº¡m bá»• sung cho Ã´ tÃ´
        {
            "violation_description": "Cháº¡y quÃ¡ tá»‘c Ä‘á»™ cho phÃ©p tá»« 5-10 km/h trong khu dÃ¢n cÆ°",
            "legal_basis": "Nghá»‹ Ä‘á»‹nh 100/2019/NÄ-CP, Äiá»u 5, Khoáº£n 2",
            "fine_amount_min": 400000,
            "fine_amount_max": 600000,
            "vehicle_type": "Ã” tÃ´",
            "category": "Ã” tÃ´ - Vi pháº¡m chung",
            "severity_level": "Nháº¹"
        },
        {
            "violation_description": "KhÃ´ng báº­t Ä‘Ã¨n xi nhan khi chuyá»ƒn lÃ n Ä‘Æ°á»ng",
            "legal_basis": "Nghá»‹ Ä‘á»‹nh 100/2019/NÄ-CP, Äiá»u 5, Khoáº£n 10", 
            "fine_amount_min": 800000,
            "fine_amount_max": 1200000,
            "vehicle_type": "Ã” tÃ´",
            "category": "Ã” tÃ³ - Vi pháº¡m chung",
            "severity_level": "Nháº¹"
        },
        # Vi pháº¡m bá»• sung cho mÃ´ tÃ´
        {
            "violation_description": "Cho ngÆ°á»i ngá»“i sau khÃ´ng Ä‘á»™i mÅ© báº£o hiá»ƒm",
            "legal_basis": "Nghá»‹ Ä‘á»‹nh 100/2019/NÄ-CP, Äiá»u 6, Khoáº£n 1",
            "fine_amount_min": 200000,
            "fine_amount_max": 300000,
            "vehicle_type": "MÃ´ tÃ´/Xe gáº¯n mÃ¡y",
            "category": "MÃ´ tÃ´, xe gáº¯n mÃ¡y",
            "severity_level": "Ráº¥t nháº¹"
        },
        # ThÃªm nhiá»u vi pháº¡m khÃ¡c...
    ]
    
    return additional_violations

def create_complete_dataset() -> pd.DataFrame:
    """Táº¡o dataset hoÃ n chá»‰nh tá»« táº¥t cáº£ cÃ¡c nghá»‹ Ä‘á»‹nh"""
    
    # Load CSV hiá»‡n táº¡i trÆ°á»›c (base dataset)
    print("ğŸ“Š Loading current CSV dataset...")
    current_df = load_current_csv()
    print(f"   Current CSV: {len(current_df)} violations")
    
    # Load táº¥t cáº£ legal documents
    all_documents = load_all_legal_documents()
    
    all_violations = []
    
    # Extract tá»« document chÃ­nh (ND 100/2019) - chá»‰ Ä‘á»ƒ validation/enhancement
    if "ND_100_2019" in all_documents:
        print("ğŸ“– Processing ND 100/2019...")
        base_violations = extract_violations_from_json(all_documents["ND_100_2019"])
        print(f"   + {len(base_violations)} violations from base document (for reference)")
    
    # Extract tá»« amendment documents - chá»‰ thÃªm nhá»¯ng cÃ¡i má»›i/updated
    amendment_docs = ["ND_123_2021", "ND_168_2024"]
    for doc_name in amendment_docs:
        if doc_name in all_documents:
            print(f"ğŸ“– Processing {doc_name}...")
            amendment_violations = extract_violations_from_amendment(all_documents[doc_name], doc_name)
            all_violations.extend(amendment_violations)
            print(f"   + {len(amendment_violations)} NEW violations from {doc_name}")
    
    # Start with current CSV as base
    combined_df = current_df.copy()
    
    # Add amendment violations only
    if all_violations:
        amendment_df = pd.DataFrame(all_violations)
        
        # Ensure column compatibility
        for col in combined_df.columns:
            if col not in amendment_df.columns:
                amendment_df[col] = ""
        
        for col in amendment_df.columns:
            if col not in combined_df.columns:
                combined_df[col] = ""
        
        # Append amendment violations (khÃ´ng replace existing)
        combined_df = pd.concat([combined_df, amendment_df], ignore_index=True)
        print(f"   Added {len(amendment_df)} amendment violations to existing dataset")
    
    # Update existing violations based on amendments if needed
    amendment_doc = all_documents.get("ND_123_2021", {})
    if amendment_doc:
        combined_df = apply_amendments_to_existing_violations(combined_df, amendment_doc)
    
    # Normalize data
    combined_df = combined_df.fillna("")
    
    # Reset violation IDs to be sequential
    combined_df = combined_df.reset_index(drop=True)
    combined_df['violation_id'] = range(1, len(combined_df) + 1)
    
    print(f"\nğŸ“Š Final Dataset Summary:")
    print(f"   Total violations: {len(combined_df)}")
    
    # Count by source document
    if 'source_document' in combined_df.columns:
        source_counts = combined_df['source_document'].fillna('Original CSV').value_counts()
        for source, count in source_counts.items():
            print(f"   - {source}: {count} violations")
    else:
        print(f"   - Original CSV: {len(current_df)} violations")
        print(f"   - Amendments: {len(combined_df) - len(current_df)} violations")
    
    return combined_df

def apply_amendments_to_existing_violations(df: pd.DataFrame, amendment_doc: Dict) -> pd.DataFrame:
    """Apply amendment updates to existing violations in CSV"""
    
    violation_updates = amendment_doc.get('violation_updates', [])
    
    print(f"ğŸ”„ Applying {len(violation_updates)} amendment updates to existing violations...")
    
    updates_applied = 0
    
    for update in violation_updates:
        violation_code = update.get('violation_code', '')
        
        # Find violations in CSV that match this code or description
        for idx, row in df.iterrows():
            # Check if this violation should be updated
            if (violation_code in str(row.get('legal_basis', '')) or 
                violation_code in str(row.get('violation_description', ''))):
                
                # Update fine amounts
                if 'after_123' in update:
                    after_data = update['after_123']
                    if 'fine_range' in after_data:
                        fine_min, fine_max = parse_fine_range(after_data['fine_range'])
                        if fine_min > 0:  # Only update if we have valid fine data
                            df.at[idx, 'fine_min'] = fine_min
                            df.at[idx, 'fine_max'] = fine_max
                            df.at[idx, 'fine_amount_min'] = fine_min
                            df.at[idx, 'fine_amount_max'] = fine_max
                            
                            # Add amendment metadata
                            df.at[idx, 'additional_measures'] = "; ".join(after_data.get('additional_measures', []))
                            df.at[idx, 'source_document'] = 'Updated by ND 123/2021'
                            
                            updates_applied += 1
                            break
    
    print(f"   Applied {updates_applied} updates to existing violations")
    return df
    
    # Äáº£m báº£o cÃ³ Ä‘Ãºng 299 dÃ²ng
    if len(combined_df) > 299:
        combined_df = combined_df.head(299)
    
    # Sáº¯p xáº¿p theo article_number vÃ  section
    combined_df = combined_df.sort_values(['article_number', 'section'], na_position='last')
    combined_df = combined_df.reset_index(drop=True)
    combined_df['violation_id'] = combined_df.index + 1
    
    return combined_df

def save_updated_dataset(df: pd.DataFrame) -> None:
    """LÆ°u dataset Ä‘Ã£ cáº­p nháº­t"""
    
    # Backup file cÅ©
    import shutil
    csv_path = "../raw/violations_dataset/traffic_violations_extended.csv"
    backup_path = "../raw/violations_dataset/traffic_violations_extended_backup.csv"
    
    shutil.copy2(csv_path, backup_path)
    print(f"ğŸ”„ ÄÃ£ backup dataset cÅ©: {backup_path}")
    
    # LÆ°u dataset má»›i
    df.to_csv(csv_path, index=False, encoding='utf-8')
    print(f"âœ… ÄÃ£ cáº­p nháº­t dataset: {csv_path}")
    
    # Táº¡o thÃªm file thá»‘ng kÃª
    stats_path = "../raw/violations_dataset/dataset_statistics.txt"
    
    with open(stats_path, 'w', encoding='utf-8') as f:
        f.write("THá»NG KÃŠ DATASET VI PHáº M GIAO THÃ”NG\n")
        f.write("=" * 50 + "\n")
        f.write(f"Tá»•ng sá»‘ vi pháº¡m: {len(df)}\n")
        f.write(f"Sá»‘ categories: {df['category'].nunique()}\n")
        f.write(f"Sá»‘ loáº¡i phÆ°Æ¡ng tiá»‡n: {df['vehicle_type'].nunique()}\n")
        f.write(f"Má»©c pháº¡t tá»‘i thiá»ƒu: {df['fine_amount_min'].min():,} VNÄ\n")
        f.write(f"Má»©c pháº¡t tá»‘i Ä‘a: {df['fine_amount_max'].max():,} VNÄ\n")
        f.write("\nPhÃ¢n bá»‘ theo category:\n")
        f.write(df['category'].value_counts().to_string())
        f.write("\n\nPhÃ¢n bá»‘ theo má»©c Ä‘á»™ nghiÃªm trá»ng:\n")
        f.write(df['severity_level'].value_counts().to_string())
    
    print(f"ğŸ“Š ÄÃ£ táº¡o file thá»‘ng kÃª: {stats_path}")

def validate_dataset(df: pd.DataFrame) -> None:
    """Kiá»ƒm tra tÃ­nh toÃ n váº¹n cá»§a dataset"""
    print("\nğŸ“‹ KIá»‚M TRA DATASET:")
    print("=" * 40)
    
    print(f"ğŸ“Š Tá»•ng sá»‘ vi pháº¡m: {len(df)}")
    print(f"ğŸ“– Sá»‘ cá»™t: {len(df.columns)}")
    print(f"ğŸ“‹ Columns: {list(df.columns)}")
    
    # Kiá»ƒm tra dá»¯ liá»‡u trá»‘ng
    null_counts = df.isnull().sum()
    if null_counts.sum() > 0:
        print("\nâš ï¸ Cá»™t cÃ³ dá»¯ liá»‡u trá»‘ng:")
        for col, count in null_counts.items():
            if count > 0:
                print(f"   {col}: {count} dÃ²ng")
    else:
        print("âœ… KhÃ´ng cÃ³ dá»¯ liá»‡u trá»‘ng")
    
    # Kiá»ƒm tra legal basis - xá»­ lÃ½ an toÃ n
    try:
        if 'article_number' in df.columns:
            unique_articles = df['article_number'].nunique()
            print(f"\nğŸ“ Sá»‘ Ä‘iá»u Ä‘Æ°á»£c tham chiáº¿u: {unique_articles}")
    except:
        print("\nğŸ“ KhÃ´ng thá»ƒ kiá»ƒm tra sá»‘ Ä‘iá»u tham chiáº¿u")
    
    # Kiá»ƒm tra má»©c pháº¡t - xá»­ lÃ½ an toÃ n kiá»ƒu dá»¯ liá»‡u
    try:
        # Chuyá»ƒn Ä‘á»•i sang numeric náº¿u cáº§n
        if 'fine_amount_min' in df.columns:
            df['fine_amount_min'] = pd.to_numeric(df['fine_amount_min'], errors='coerce').fillna(0)
        if 'fine_amount_max' in df.columns:
            df['fine_amount_max'] = pd.to_numeric(df['fine_amount_max'], errors='coerce').fillna(0)
        
        valid_fines = df[(df['fine_amount_min'] > 0) | (df['fine_amount_max'] > 0)]
        print(f"ğŸ’° Sá»‘ vi pháº¡m cÃ³ má»©c pháº¡t: {len(valid_fines)}")
    except Exception as e:
        print(f"ğŸ’° KhÃ´ng thá»ƒ kiá»ƒm tra má»©c pháº¡t: {e}")
    
    # PhÃ¢n bá»‘ theo loáº¡i phÆ°Æ¡ng tiá»‡n
    try:
        if 'vehicle_type' in df.columns:
            print(f"\nğŸš— PhÃ¢n bá»‘ theo loáº¡i phÆ°Æ¡ng tiá»‡n:")
            vehicle_counts = df['vehicle_type'].value_counts()
            for vehicle, count in vehicle_counts.items():
                print(f"   {vehicle}: {count} vi pháº¡m")
    except:
        print("\nğŸš— KhÃ´ng thá»ƒ kiá»ƒm tra phÃ¢n bá»‘ theo loáº¡i phÆ°Æ¡ng tiá»‡n")
    
    # PhÃ¢n bá»‘ theo má»©c Ä‘á»™ nghiÃªm trá»ng
    try:
        if 'severity_level' in df.columns:
            print(f"\nâš–ï¸ PhÃ¢n bá»‘ theo má»©c Ä‘á»™ nghiÃªm trá»ng:")
            severity_counts = df['severity_level'].value_counts()
            for severity, count in severity_counts.items():
                print(f"   {severity}: {count} vi pháº¡m")
    except:
        print("\nâš–ï¸ KhÃ´ng thá»ƒ kiá»ƒm tra phÃ¢n bá»‘ theo má»©c Ä‘á»™ nghiÃªm trá»ng")

if __name__ == "__main__":
    print("ğŸš€ Báº®T Äáº¦U Cáº¬P NHáº¬T DATASET VI PHáº M")
    print("=" * 50)
    
    # Táº¡o dataset hoÃ n chá»‰nh
    print("ğŸ“ Äang táº¡o dataset tá»« JSON vÃ  CSV...")
    complete_df = create_complete_dataset()
    
    # Kiá»ƒm tra dataset
    validate_dataset(complete_df)
    
    # Há»i ngÆ°á»i dÃ¹ng cÃ³ muá»‘n lÆ°u khÃ´ng
    save_choice = input("\nâ“ Báº¡n cÃ³ muá»‘n lÆ°u dataset cáº­p nháº­t? (y/n): ").lower().strip()
    if save_choice in ['y', 'yes', 'cÃ³']:
        save_updated_dataset(complete_df)
        print("âœ… HoÃ n thÃ nh cáº­p nháº­t dataset!")
    else:
        print("â„¹ï¸ Dataset khÃ´ng Ä‘Æ°á»£c lÆ°u.")
    
    print("\nğŸ‰ HOÃ€N THÃ€NH!")
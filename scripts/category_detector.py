#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ALL-IN-ONE Category Detection and Processing System
Merges all categorization functionality into a single script
"""

import json
import re
import os
import hashlib
from datetime import datetime
from collections import Counter

class VehicleCategoryDetector:
    """Enhanced vehicle type and category detection system"""
    
    def __init__(self):
        self.vehicle_patterns = {
            # Primary vehicle types
            "Xe Ã´ tÃ´": {
                "keywords": ["Ã´ tÃ´", "xe hÆ¡i", "car", "xe con"],
                "patterns": [r"xe\s+Ã´\s+tÃ´", r"Ã´\s+tÃ´", r"xe\s+hÆ¡i"],
                "priority": 10
            },
            "Xe mÃ´ tÃ´, xe mÃ¡y": {
                "keywords": ["mÃ´ tÃ´", "xe mÃ¡y", "xe gáº¯n mÃ¡y", "motorbike", "motorcycle"],
                "patterns": [r"xe\s+mÃ´\s+tÃ´", r"mÃ´\s+tÃ´", r"xe\s+mÃ¡y", r"xe\s+gáº¯n\s+mÃ¡y"],
                "priority": 10
            },
            "Xe thÃ´ sÆ¡": {
                "keywords": ["xe thÃ´ sÆ¡", "xe bÃ²", "sá»©c ngÆ°á»i", "sá»©c sÃºc váº­t", "sÃºc váº­t"],
                "patterns": [r"xe\s+thÃ´\s+sÆ¡", r"xe\s+bÃ²", r"sá»©c\s+ngÆ°á»i", r"sá»©c\s+sÃºc\s+váº­t", r"sÃºc\s+váº­t"],
                "priority": 10
            },
            
            # Commercial vehicles
            "Xe khÃ¡ch, xe buÃ½t": {
                "keywords": ["xe khÃ¡ch", "xe buÃ½t", "xe bus", "bus", "buÃ½t"],
                "patterns": [r"xe\s+khÃ¡ch", r"xe\s+buÃ½t", r"xe\s+bus"],
                "priority": 9
            },
            "Xe táº£i, container": {
                "keywords": ["xe táº£i", "container", "xe container", "táº£i"],
                "patterns": [r"xe\s+táº£i", r"container"],
                "priority": 9
            },
            "RÆ¡ moÃ³c, sÆ¡ mi rÆ¡ moÃ³c": {
                "keywords": ["rÆ¡ moÃ³c", "moÃ³c", "sÆ¡ mi rÆ¡ moÃ³c", "trailer"],
                "patterns": [r"rÆ¡\s+moÃ³c", r"sÆ¡\s+mi\s+rÆ¡\s+moÃ³c"],
                "priority": 9
            },
            
            # Special vehicles
            "Xe chuyÃªn dá»¥ng": {
                "keywords": ["cá»©u thÆ°Æ¡ng", "cá»©u há»a", "xe cá»©u", "xe cáº©u", "xe ben", "xe bá»“n", "chuyÃªn dá»¥ng"],
                "patterns": [r"cá»©u\s+thÆ°Æ¡ng", r"cá»©u\s+há»a", r"xe\s+cá»©u", r"xe\s+cáº©u", r"xe\s+ben", r"xe\s+bá»“n", r"chuyÃªn\s+dá»¥ng"],
                "priority": 9
            },
            "Taxi, xe du lá»‹ch": {
                "keywords": ["taxi", "xe taxi", "xe du lá»‹ch", "du lá»‹ch"],
                "patterns": [r"taxi", r"xe\s+taxi", r"xe\s+du\s+lá»‹ch"],
                "priority": 8
            },
            
            # Other vehicles
            "Xe Ä‘áº¡p": {
                "keywords": ["xe Ä‘áº¡p", "bicycle", "bike", "Ä‘áº¡p xe"],
                "patterns": [r"xe\s+Ä‘áº¡p", r"Ä‘áº¡p\s+xe"],
                "priority": 8
            },
            "NgÆ°á»i Ä‘i bá»™": {
                "keywords": ["ngÆ°á»i Ä‘i bá»™", "pedestrian", "Ä‘i bá»™"],
                "patterns": [r"ngÆ°á»i\s+Ä‘i\s+bá»™", r"Ä‘i\s+bá»™"],
                "priority": 8
            },
            "Xe lÄƒn": {
                "keywords": ["xe lÄƒn", "wheelchair", "ngÆ°á»i khuyáº¿t táº­t"],
                "patterns": [r"xe\s+lÄƒn", r"ngÆ°á»i\s+khuyáº¿t\s+táº­t"],
                "priority": 8
            },
            
            # Railway and water transport
            "TÃ u há»a, Ä‘Æ°á»ng sáº¯t": {
                "keywords": ["tÃ u há»a", "Ä‘Æ°á»ng sáº¯t", "railway", "train", "tÃ u lá»­a", "toa tÃ u"],
                "patterns": [r"tÃ u\s+há»a", r"Ä‘Æ°á»ng\s+sáº¯t", r"tÃ u\s+lá»­a", r"toa\s+tÃ u"],
                "priority": 9
            },
            "TÃ u thá»§y, thuyá»n": {
                "keywords": ["tÃ u thá»§y", "thuyá»n", "boat", "ship", "thá»§y"],
                "patterns": [r"tÃ u\s+thá»§y", r"thuyá»n"],
                "priority": 8
            },
            
            # Electric and special transport
            "Xe Ä‘iá»‡n": {
                "keywords": ["xe Ä‘iá»‡n", "tram", "xe buÃ½t Ä‘iá»‡n", "xe mÃ¡y Ä‘iá»‡n"],
                "patterns": [r"xe\s+Ä‘iá»‡n", r"xe\s+buÃ½t\s+Ä‘iá»‡n", r"xe\s+mÃ¡y\s+Ä‘iá»‡n"],
                "priority": 8
            },
            "MÃ¡y kÃ©o": {
                "keywords": ["mÃ¡y kÃ©o", "tractor"],
                "patterns": [r"mÃ¡y\s+kÃ©o"],
                "priority": 8
            }
        }
        
        # Business and administrative patterns
        self.business_patterns = {
            "Kinh doanh váº­n táº£i": {
                "keywords": ["kinh doanh váº­n táº£i", "váº­n táº£i", "kinh doanh", "doanh nghiá»‡p váº­n táº£i"],
                "patterns": [r"kinh\s+doanh\s+váº­n\s+táº£i", r"doanh\s+nghiá»‡p\s+váº­n\s+táº£i"],
                "priority": 9
            },
            "ÄÃ o táº¡o lÃ¡i xe": {
                "keywords": ["Ä‘Ã o táº¡o", "há»c lÃ¡i xe", "trung tÃ¢m Ä‘Ã o táº¡o", "cÆ¡ sá»Ÿ Ä‘Ã o táº¡o"],
                "patterns": [r"Ä‘Ã o\s+táº¡o", r"há»c\s+lÃ¡i\s+xe", r"trung\s+tÃ¢m\s+Ä‘Ã o\s+táº¡o", r"cÆ¡\s+sá»Ÿ\s+Ä‘Ã o\s+táº¡o"],
                "priority": 9
            },
            "SÃ¡t háº¡ch lÃ¡i xe": {
                "keywords": ["sÃ¡t háº¡ch", "thi báº±ng lÃ¡i", "kiá»ƒm tra lÃ¡i xe"],
                "patterns": [r"sÃ¡t\s+háº¡ch", r"thi\s+báº±ng\s+lÃ¡i", r"kiá»ƒm\s+tra\s+lÃ¡i\s+xe"],
                "priority": 9
            }
        }
        
        # Fallback categories
        self.fallback_categories = {
            "Vi pháº¡m giáº¥y tá»": ["giáº¥y phÃ©p", "báº±ng lÃ¡i", "Ä‘Äƒng kÃ½", "chá»©ng nháº­n"],
            "Vi pháº¡m tá»‘c Ä‘á»™": ["tá»‘c Ä‘á»™", "cháº¡y quÃ¡", "km/h", "vÆ°á»£t tá»‘c Ä‘á»™"],
            "Vi pháº¡m tÃ­n hiá»‡u giao thÃ´ng": ["Ä‘Ã¨n Ä‘á»", "tÃ­n hiá»‡u", "hiá»‡u lá»‡nh", "biá»ƒn bÃ¡o"],
            "Vi pháº¡m vá» rÆ°á»£u bia": ["rÆ°á»£u", "bia", "cá»“n", "ná»“ng Ä‘á»™ cá»“n"],
            "Sá»­ dá»¥ng Ä‘iá»‡n thoáº¡i": ["Ä‘iá»‡n thoáº¡i", "di Ä‘á»™ng", "phone"],
            "Vi pháº¡m dá»«ng Ä‘á»— xe": ["dá»«ng xe", "Ä‘á»— xe", "parking"],
            "Vi pháº¡m mÅ© báº£o hiá»ƒm": ["mÅ© báº£o hiá»ƒm", "helmet"],
            "Vi pháº¡m dÃ¢y an toÃ n": ["dÃ¢y an toÃ n", "tháº¯t dÃ¢y", "seat belt"],
            "Vi pháº¡m chá»Ÿ ngÆ°á»i/hÃ ng": ["chá»Ÿ ngÆ°á»i", "chá»Ÿ hÃ ng", "quÃ¡ táº£i", "overload"],
            "Vi pháº¡m vÆ°á»£t xe": ["vÆ°á»£t xe", "vÆ°á»£t", "overtaking"],
            "Quáº£n lÃ½ nhÃ  nÆ°á»›c": ["cÆ¡ quan", "thanh tra", "kiá»ƒm tra", "quáº£n lÃ½ nhÃ  nÆ°á»›c"]
        }
    
    def detect_category(self, text, article_title="", article_number="", using_fallback=True):
        """Detect category for a violation text"""
        combined_text = f"{text} {article_title}".lower()
        
        # Combine all patterns
        all_patterns = {**self.vehicle_patterns, **self.business_patterns}
        
        detected_types = []
        
        # Check each pattern
        for category_type, config in all_patterns.items():
            score = 0
            
            # Check keywords
            for keyword in config["keywords"]:
                if keyword in combined_text:
                    score += 1
            
            # Check regex patterns
            for pattern in config.get("patterns", []):
                if re.search(pattern, combined_text):
                    score += 2
            
            if score > 0:
                detected_types.append({
                    "type": category_type,
                    "score": score * config["priority"],
                    "priority": config["priority"]
                })
        
        # Sort by score and priority
        detected_types.sort(key=lambda x: (x["score"], x["priority"]), reverse=True)
        
        # Return the highest scoring type, or fallback categories
        if detected_types:
            return detected_types[0]["type"]
        
        # Check fallback categories
        if using_fallback:
            for category, keywords in self.fallback_categories.items():
                if any(keyword in combined_text for keyword in keywords):
                    return category
        
            return "Vi pháº¡m khÃ¡c"
        else:
            return None

class ViolationProcessor:
    """Process violations from raw to categorized format"""
    
    def __init__(self):
        self.detector = VehicleCategoryDetector()
        self.raw_path = r"C:\Users\Mr Hieu\Documents\vietnamese-traffic-law-qa\data\raw\legal_documents\nghi_dinh_100_2019.json"
        self.processed_path = r"C:\Users\Mr Hieu\Documents\vietnamese-traffic-law-qa\data\processed\violations.json"
    
    def clean_text(self, text):
        """Clean and normalize text"""
        if not text:
            return ""
        
        text = re.sub(r'\s+', ' ', text).strip()
        text = re.sub(r'[^\w\s\-.,():;/]', '', text)
        return text
    
    def extract_fine_amounts(self, fine_range):
        """Extract min and max fine amounts from fine range string"""
        if not fine_range:
            return 0, 0, ""
        
        fine_text = fine_range.replace('VNÄ', '').strip()
        numbers = re.findall(r'(\d+(?:[.,]\d{3})*)', fine_text)
        
        if not numbers:
            return 0, 0, fine_range
        
        amounts = []
        for num in numbers:
            clean_num = num.replace('.', '').replace(',', '')
            try:
                amounts.append(int(clean_num))
            except ValueError:
                continue
        
        if len(amounts) >= 2:
            return min(amounts), max(amounts), fine_range
        elif len(amounts) == 1:
            return amounts[0], amounts[0], fine_range
        else:
            return 0, 0, fine_range
    
    def get_severity_level(self, fine_min, fine_max):
        """Determine severity based on fine amount"""
        max_fine = max(fine_min, fine_max)
        
        if max_fine == 0:
            return "KhÃ´ng xÃ¡c Ä‘á»‹nh"
        elif max_fine < 1000000:
            return "Nháº¹"
        elif max_fine < 5000000:
            return "Trung bÃ¬nh"
        elif max_fine < 20000000:
            return "Náº·ng"
        else:
            return "Ráº¥t náº·ng"
    
    def extract_keywords(self, violation_text):
        """Extract keywords for search"""
        keywords = []
        text_lower = violation_text.lower()
        
        keyword_patterns = [
            "tá»‘c Ä‘á»™", "Ä‘Ã¨n Ä‘á»", "rÆ°á»£u bia", "Ä‘iá»‡n thoáº¡i", "mÅ© báº£o hiá»ƒm",
            "dÃ¢y an toÃ n", "giáº¥y phÃ©p", "vÆ°á»£t xe", "dá»«ng xe", "Ä‘á»— xe",
            "chá»Ÿ ngÆ°á»i", "chá»Ÿ hÃ ng", "ngÆ°á»£c chiá»u", "láº¥n lÃ n"
        ]
        
        for keyword in keyword_patterns:
            if keyword in text_lower:
                keywords.append(keyword)
        
        return keywords
    
    def create_violation_hash(self, violation_text, article, section):
        """Create hash for duplicate detection"""
        content = f"{violation_text}_{article}_{section}".lower()
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def process_raw_to_violations(self):
        """Main processing function from raw to violations"""
        
        print("ğŸ”„ Processing raw legal documents to categorized violations...")
        
        # Load raw data
        try:
            with open(self.raw_path, 'r', encoding='utf-8') as f:
                raw_data = json.load(f)
        except Exception as e:
            print(f"âŒ Error loading raw data: {e}")
            return False
        
        processed_violations = []
        seen_hashes = set()
        violation_id = 1
        category_stats = Counter()
        
        # Process each article
        for article_key, article_data in raw_data.get('key_articles', {}).items():
            if not isinstance(article_data, dict) or 'sections' not in article_data:
                continue
            
            article_title = article_data.get('title', '')
            article_number = article_key.replace('dieu_', '')
            
            # Process each section
            for section in article_data.get('sections', []):
                if not isinstance(section, dict):
                    continue
                
                section_name = section.get('section', '')
                fine_range = section.get('fine_range', '')
                additional_measures = section.get('additional_measures', [])
                
                # Extract fine amounts
                fine_min, fine_max, fine_text = self.extract_fine_amounts(fine_range)
                
                # Skip if no valid fine amount and no additional measures
                if fine_min == 0 and fine_max == 0 and not additional_measures:
                    continue
                
                # Process each violation
                for violation_text in section.get('violations', []):
                    if not violation_text or not violation_text.strip():
                        continue
                    
                    violation_text = self.clean_text(violation_text)
                    
                    if len(violation_text) < 10:
                        continue
                    
                    # Check for duplicates
                    violation_hash = self.create_violation_hash(violation_text, f"Äiá»u {article_number}", section_name)
                    if violation_hash in seen_hashes:
                        continue
                    seen_hashes.add(violation_hash)
                    
                    # Detect category
                    category = self.detector.detect_category(violation_text, article_title, article_number)
                    
                    # Skip uncategorized violations with no penalty
                    if category == "Vi pháº¡m khÃ¡c" and fine_min == 0 and fine_max == 0:
                        continue
                    
                    # Create violation record
                    violation_record = {
                        "id": violation_id,
                        "description": violation_text,
                        "category": category,
                        "penalty": {
                            "fine_min": fine_min,
                            "fine_max": fine_max,
                            "currency": "VNÄ",
                            "fine_text": fine_text if fine_text else f"{fine_min:,} - {fine_max:,} VNÄ".replace(",", ".")
                        },
                        "additional_measures": additional_measures,
                        "legal_basis": {
                            "article": f"Äiá»u {article_number}",
                            "section": section_name,
                            "document": "Nghá»‹ Ä‘á»‹nh 100/2019/NÄ-CP",
                            "full_reference": f"Nghá»‹ Ä‘á»‹nh 100/2019/NÄ-CP, Äiá»u {article_number}, {section_name}"
                        },
                        "severity": self.get_severity_level(fine_min, fine_max),
                        "keywords": self.extract_keywords(violation_text),
                        "search_text": f"{violation_text} {category} Äiá»u {article_number} {article_title}",
                        "metadata": {
                            "source": "ND100-2019.docx",
                            "processed_date": datetime.now().isoformat(),
                            "categorization_method": "enhanced_vehicle_detection_v3"
                        }
                    }
                    
                    processed_violations.append(violation_record)
                    category_stats[category] += 1
                    violation_id += 1
        
        # Create final output
        output_data = {
            "metadata": {
                "total_violations": len(processed_violations),
                "processed_date": datetime.now().isoformat(),
                "source_documents": ["Nghá»‹ Ä‘á»‹nh 100/2019/NÄ-CP"],
                "data_sources": [self.raw_path],
                "processing_pipeline": "raw->processed (enhanced_direct)",
                "categorization_method": "enhanced_vehicle_detection_v3",
                "validation_summary": {
                    "total_violations": len(processed_violations),
                    "valid_legal_references": len(processed_violations),
                    "duplicates_removed": len(seen_hashes) - len(processed_violations),
                    "categories": len(category_stats)
                },
                "categories": list(category_stats.keys()),
                "severity_levels": list(set(v["severity"] for v in processed_violations))
            },
            "violations": processed_violations
        }
        
        # Save processed data
        try:
            # Backup existing file if it exists
            if os.path.exists(self.processed_path):
                backup_path = self.processed_path.replace(".json", f"_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
                os.rename(self.processed_path, backup_path)
                print(f"ğŸ“¦ Backed up existing file to: {os.path.basename(backup_path)}")
            
            with open(self.processed_path, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… Successfully processed {len(processed_violations)} violations")
            print(f"ğŸ“Š Categories detected: {len(category_stats)}")
            
            # Show category breakdown
            print(f"\nğŸ“‹ Category breakdown:")
            for category, count in sorted(category_stats.items(), key=lambda x: x[1], reverse=True):
                percentage = (count / len(processed_violations)) * 100
                print(f"   {category}: {count} ({percentage:.1f}%)")
            
            return True
            
        except Exception as e:
            print(f"âŒ Error saving processed data: {e}")
            return False

class CategoryAnalyzer:
    """Analyze and report on categorization results"""
    
    def __init__(self, processed_path):
        self.processed_path = processed_path
    
    def analyze_results(self):
        """Analyze categorization results"""
        
        try:
            with open(self.processed_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except Exception as e:
            print(f"âŒ Error loading processed data: {e}")
            return
        
        violations = data.get('violations', [])
        metadata = data.get('metadata', {})
        
        print(f"\nğŸ“Š CATEGORIZATION ANALYSIS REPORT")
        print("=" * 50)
        print(f"ğŸ“„ Total violations: {len(violations)}")
        print(f"ğŸ“… Processed: {metadata.get('processed_date', 'Unknown')}")
        print(f"ğŸ”„ Method: {metadata.get('categorization_method', 'Unknown')}")
        
        categories = Counter(v.get('category') for v in violations)
        
        # Separate vehicle vs other categories
        vehicle_categories = {}
        other_categories = {}
        
        for category, count in categories.items():
            if any(vehicle in category.lower() for vehicle in [
                'xe Ã´ tÃ´', 'xe mÃ´ tÃ´', 'xe mÃ¡y', 'xe thÃ´ sÆ¡', 'xe Ä‘áº¡p', 
                'ngÆ°á»i Ä‘i bá»™', 'xe lÄƒn', 'tÃ u há»a', 'Ä‘Æ°á»ng sáº¯t', 'xe Ä‘iá»‡n',
                'xe táº£i', 'xe khÃ¡ch', 'xe buÃ½t', 'taxi', 'rÆ¡ moÃ³c', 'tÃ u thá»§y'
            ]):
                vehicle_categories[category] = count
            else:
                other_categories[category] = count
        
        print(f"\nğŸš— Vehicle-specific categories ({len(vehicle_categories)} types):")
        for category, count in sorted(vehicle_categories.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / len(violations)) * 100
            print(f"   {category}: {count} ({percentage:.1f}%)")
        
        print(f"\nğŸ“‹ Other categories ({len(other_categories)} types):")
        for category, count in sorted(other_categories.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / len(violations)) * 100
            print(f"   {category}: {count} ({percentage:.1f}%)")
        
        # Summary statistics
        total_vehicle = sum(vehicle_categories.values())
        total_other = sum(other_categories.values())
        
        print(f"\nğŸ“ˆ Summary:")
        print(f"   ğŸš— Vehicle-specific: {total_vehicle} ({(total_vehicle/len(violations)*100):.1f}%)")
        print(f"   ğŸ“‹ Other violations: {total_other} ({(total_other/len(violations)*100):.1f}%)")
        print(f"   ğŸ·ï¸  Total categories: {len(categories)}")
        
        return {
            'total_violations': len(violations),
            'vehicle_categories': len(vehicle_categories),
            'other_categories': len(other_categories),
            'vehicle_percentage': (total_vehicle/len(violations)*100)
        }

def main():
    """Main function to run the complete categorization process"""
    
    print("ğŸš— ENHANCED CATEGORY DETECTION SYSTEM")
    print("=" * 60)
    print("ğŸ”„ Processing raw legal documents â†’ categorized violations")
    
    # Initialize processor
    processor = ViolationProcessor()
    
    # Process raw data to violations
    success = processor.process_raw_to_violations()
    
    if success:
        # Analyze results
        analyzer = CategoryAnalyzer(processor.processed_path)
        stats = analyzer.analyze_results()
        
        print(f"\nğŸ‰ CATEGORIZATION COMPLETED SUCCESSFULLY!")
        print("=" * 50)
        print(f"âœ… {stats['total_violations']} violations processed and categorized")
        print(f"ğŸš— {stats['vehicle_categories']} vehicle-specific categories detected")
        print(f"ğŸ“‹ {stats['other_categories']} other categories detected")
        print(f"ğŸ¯ {stats['vehicle_percentage']:.1f}% are vehicle-specific violations")
        
        print(f"\nğŸ“ Output saved to: data/processed/violations.json")
        print(f"ğŸ” Ready for Q&A system usage!")
    else:
        print(f"\nâŒ Categorization failed. Please check the error messages above.")

if __name__ == "__main__":
    main()